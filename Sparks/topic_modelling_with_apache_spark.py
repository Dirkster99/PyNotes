# -*- coding: utf-8 -*-
"""Topic Modelling with Apache Spark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W5sciKse0NLnZJZXXML98pT0uqivvNrB

# **This Notebook Has Instruction To Setup Spark Environment Inside The Google Colab Environment.**

https://medium.com/ai-for-real/spark-with-google-colab-fd6881e6acd3

*To install Spark we would need to install JDK inside the google colab environment*

*We Will install JDK 8 using apt-get install inside the google colab*
"""

!apt-get install openjdk-8-jdk-headless

!java --version

!wget https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz

!tar xvf spark-2.4.5-bin-hadoop2.7.tgz

!ls

!ls spark-2.4.5-bin-hadoop2.7

!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.5-bin-hadoop2.7"

import findspark
findspark.init()

findspark.find()

"""Content below was taken from this YouTube video

**Topic Modelling from Scratch using Python and Apache Spark**
https://www.youtube.com/watch?v=stBVE0tDtiQ

by AIEngineering
"""

# http://qwone.com/~jason/20Newsgroups/
from sklearn.datasets import fetch_20newsgroups

newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')

from pyspark.sql.types import *
from pyspark.sql.functions import *

# Create a spark session
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

from nltk.stem import WordNetLemmatizer, PorterStemmer
import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

type(newsgroups_train)

import pandas as pd
df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()])

df

# Transpose dataframe such that columns become rows in comparison to last output
df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T
df.columns = ['text', 'target']

targets = pd.DataFrame( newsgroups_train.target_names)
targets.columns = ['title']

ngout = pd.merge(df, targets, left_on='target', right_index=True)

display(ngout)

sdf = spark.createDataFrame(ngout)

sdf.head(10)

# Lets get ride of the Email header by splitting the document along double enter characters '\n\n'
from pyspark.sql.functions import split
from pyspark.sql.functions import monotonically_increasing_id, col

# create a new column 'text_sep' by splitting the content of the 'text' column by 2 newline characters
# put the second and third text part into seperate columns text_sep[1] and text_sep[2] effectively removing the email header
# and create a unique id for every document
sdf=sdf.withColumn("text_sep", split(sdf.text, '\n\n')).\
    select(col('text'), col('target'), col('title'), \
           col('text_sep').getItem(1), col('text_sep').getItem(2)).withColumn('id', monotonically_increasing_id())

sdf.head(10)

sdf.printSchema()

sdf.count()

temp_table_name = 'newsgroup'

sdf.createOrReplaceTempView(temp_table_name)

spark.sql('select * from ' + temp_table_name).show()

# Lets check for nulls because concatinating null with something else yields null :-(
spark.sql('select count(*) from ' + temp_table_name + ' where `text_sep[2]` is null').show()

# First document may not be null but can be empty which is also not too useful
spark.sql('select count(*) from ' + temp_table_name + ' where `text_sep[1]` =\'\'').show()

from pyspark.sql.types import FloatType
import re

def clean_text(in_string):
  remove_email=re.sub('\S*@\S*\s?', '', in_string)  #remove emails in strings if there are any
  remove_nl=re.sub('\s+', ' ', remove_email)        #remove multiple whitespace and replace with 1 space character
  remove_othr=re.sub("\'|\>|\:|\-", "", remove_nl)  #remove special characters ' or > or : or -
  return remove_othr

spark.udf.register('clean', clean_text)

spark.sql('select clean(CASE when `text_sep[2]` is null then `text_sep[1]` when `text_sep[1]` = \'\' then `text_sep[2]` else ' \
          + 'CONCAT(`text_sep[1]`, \' \', `text_sep[2]`) END) as text, target, title, id '
          + 'FROM ' + temp_table_name + ' where `text_sep[2]` is not null and `text_sep[1]` <> \'\'').show(20, False)

sdf=spark.sql('select clean(CASE when `text_sep[2]` is null then `text_sep[1]` when `text_sep[1]` = \'\' then `text_sep[2]` else ' \
          + 'CONCAT(`text_sep[1]`, \' \', `text_sep[2]`) END) as text, target, title, id '
          + 'FROM ' + temp_table_name + ' where `text_sep[2]` is not null and `text_sep[1]` <> \'\'')

sdf.head(10)

sdf.count()

sdf.filter('id = 17' ).show(100, False)

from pyspark.sql.functions import col, length
sdf.where(length(col('text')) < 100 ).show(20, False)

sdf=sdf.where(length(col('text')) > 100 )

from pyspark.ml.feature import RegexTokenizer, StopWordsRemover
tokenizer=RegexTokenizer(inputCol='text', outputCol='tokens', pattern='\\W+', minTokenLength=4, toLowercase=True)
tokenized=tokenizer.transform(sdf)

tokenized.show(10, False)

spremover = StopWordsRemover(inputCol='tokens', outputCol='spfiltered')
spremoved = spremover.transform(tokenized)

spremoved.select('tokens', 'spfiltered').show(10, False)

porter = PorterStemmer()
lemma = WordNetLemmatizer()

def word_tokenize(text):
  #print(text)
  pos = nltk.pos_tag(text)
  final = [lemma.lemmatize(word[0]) if (lemma.lemmatize(word[0]).endswith(('e', 'ion')) or len(word[0]) < 4) else porter.stem(word[0]) for word in pos]
  return final

spremoved.printSchema()

#spremoved.withColumn('stemmed', [tokenize(words) for words in spremoved
#http://text-processing.com/
#http://text-processing.com/demo/stem/
#
#      tup[1]  tup[2] tup[3]      tup[5]
# Take target, title, id      and spfiltered
#
stemmed = spremoved.rdd.map(lambda tup: (tup[1], tup[2], tup[2], word_tokenize(tup[5])))

stemmed.collect()

news_df = stemmed.toDF(schema=['target', 'title', 'id', 'word'])

news_df.show(10, False)

spwordlist = ["article", "write", "entry", "date", "udel", "said", "tell", "think"
            , "know", "just", "isnt", "line", "like", "does", "going", "make", "thanks", "also"]

spremover1 = StopWordsRemover(inputCol='word', outputCol="word_new", stopWords=spwordlist)
news_df = spremover1.transform(news_df)

news_df.select("word", "word_new").show(10, False)

df_explode = news_df.withColumn('word_new', explode('word_new'))

# convert list of words into rows to do frequencies
df_explode.show(10, False)

df_explode.createOrReplaceTempView('topwords')

spark.sql('select word_new, count(*) as freq from topwords group by word_new order by freq desc').show(10, False)

from pyspark.ml.feature import CountVectorizer
cv = CountVectorizer(inputCol='word_new', outputCol='rawFeatures', vocabSize=10000, minDF=5)
cvmodel = cv.fit(news_df)
featurizedData=cvmodel.transform(news_df)

featurizedData.show(10, False)

vocab = cvmodel.vocabulary
vocab_broadcast = sc.broadcast(vocab)
vocab

from pyspark.ml.feature import IDF
idf = IDF(inputCol='rawFeatures', outputCol='features')
idfModel = idf.fit(featurizedData)
rescaledData = idfModel.transform(featurizedData)

rescaledData.show(10, False)

rescaledData.printSchema()

corpus = rescaledData.select('id', 'features').cache()
corpus.show(10, False)

from pyspark.ml.clustering import LDA

lda = LDA(k=20, maxIter=50, optimizer='em')
model = lda.fit(corpus)

ll = model.logLikelihood(corpus)
lp = model.logPerplexity(corpus)
print("The lower bound on the log likelyhood of the entire corpus:" + str(ll))
print("The upper bound on perplexity:" + str(lp))

topicwords=20
topics = model.describeTopics(topicwords)
print("The topics described by their top weighted terms:")
topics.show(10, False)

tRDD = topics.rdd.map(list)

topics.printSchema()

tRDD.collect()

def topic_vocab(topic):
  print(topic)
  topicNum = topic[0]
  terms = topic[1]
  weight = topic[2]
  result = []
  for i in range(topicwords):
    term = vocab[terms[i]]
    out = str(topicNum)+","+term+","+str(weight[i])
    result.append(out)
  return result

tRDD.map(lambda topic: topic_vocab(topic)).collect()

topic = tRDD.map(lambda topic: topic_vocab(topic))

ng_pd = spark.createDataFrame(topic).toPandas()

ng_pd.T

