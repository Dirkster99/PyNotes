{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Kopie von 20 Sentiment Analysis in 10 Minutes with BERT-TensorFlow-SoftMax-LocalModel.ipynb","provenance":[{"file_id":"1uQu6abw5foeOesLeAfozoigr7E3KBtLz","timestamp":1614678741633}],"collapsed_sections":[],"authorship_tag":"ABX9TyMbTnL51jav0T5CfBIWJ26W"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"D0lTwJkEZzlE"},"source":["# Sentiment Analysis in 10 Minutes with BERT and TensorFlow\r\n","- Original Article  \r\n","  https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671"]},{"cell_type":"markdown","metadata":{"id":"uaywxYQ7Owcs"},"source":["- Data Source - Stanford Data Repository:  \r\n","  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\r\n","\r\n","- https://ai.stanford.edu/~amaas/data/sentiment/"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mpyyhRtWZxzD","executionInfo":{"status":"ok","timestamp":1614676190678,"user_tz":-60,"elapsed":6976,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"f5d0191a-c9be-424d-afcb-f054a54c266f"},"source":["pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 16.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 48.9MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 52.7MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=b71e9ae098a3b78404fd9edb662496627194e2143832188ebecfff815f7ea47c\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4wlpTiHEUbLy","executionInfo":{"status":"ok","timestamp":1614676236377,"user_tz":-60,"elapsed":52662,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"46275977-7a13-406c-e910-2c4e0b3d2707"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-Pbjz0hNU920","executionInfo":{"status":"ok","timestamp":1614676236381,"user_tz":-60,"elapsed":52660,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}}},"source":["myModelPath = '/gdrive/MyDrive/Colab Notebooks/Transformers/LocalModelUsage/bert-base-uncased/'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L0v09ziQUbL1","executionInfo":{"status":"ok","timestamp":1614676237187,"user_tz":-60,"elapsed":53455,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"b542ec32-55aa-44cd-b0fa-c455617e598e"},"source":["!ls {myModelPath.replace(' ', '\\ ')} -lh"],"execution_count":4,"outputs":[{"output_type":"stream","text":["total 1.5G\n","-rw------- 1 root root  433 Feb 23 18:20 config.json\n","-rw------- 1 root root 421M Feb 23 18:20 pytorch_model.bin\n","-rw------- 1 root root 8.8K Feb 23 18:20 README.md\n","-rw------- 1 root root 510M Feb 23 18:20 rust_model.ot\n","-rw------- 1 root root 512M Feb 23 18:20 tf_model.h5\n","-rw------- 1 root root   28 Feb 23 18:20 tokenizer_config.json\n","-rw------- 1 root root 456K Feb 23 18:20 tokenizer.json\n","-rw------- 1 root root 227K Feb 23 18:20 vocab.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jKyCtNxmaBNL","executionInfo":{"status":"ok","timestamp":1614676258202,"user_tz":-60,"elapsed":74460,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"8b3675ef-24d6-4cef-f60d-d748fc27b86f"},"source":["from transformers import BertTokenizer, TFBertForSequenceClassification\r\n","from transformers import InputExample, InputFeatures\r\n","\r\n","model = TFBertForSequenceClassification.from_pretrained(myModelPath)\r\n","tokenizer = BertTokenizer.from_pretrained(myModelPath)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at /gdrive/MyDrive/Colab Notebooks/Transformers/LocalModelUsage/bert-base-uncased/ and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"omoxwyaEaH5R","executionInfo":{"status":"ok","timestamp":1614676258209,"user_tz":-60,"elapsed":74458,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"a7e62dc3-a319-4f24-b48b-837b37515c3f"},"source":["model.summary()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","bert (TFBertMainLayer)       multiple                  109482240 \n","_________________________________________________________________\n","dropout_37 (Dropout)         multiple                  0         \n","_________________________________________________________________\n","classifier (Dense)           multiple                  1538      \n","=================================================================\n","Total params: 109,483,778\n","Trainable params: 109,483,778\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z1RiR2VPdjkT"},"source":["# IMDB Dataset\r\n","IMDB Reviews Dataset is a large movie review dataset collected and prepared by Andrew L. Maas from the popular movie rating service, IMDB. The [IMDB Reviews](https://ai.stanford.edu/~amaas/data/sentiment/) dataset is used for binary sentiment classification, whether a review is positive or negative. It contains 25,000 movie reviews for training and 25,000 for testing. All these 50,000 reviews are labeled data that may be used for supervised deep learning. \r\n","\r\n","Besides, there is an additional 50,000 unlabeled reviews that we will not use in this case study.\r\n","\r\n","In this case study, we will only use the training dataset."]},{"cell_type":"markdown","metadata":{"id":"TC1c2cSVdl0w"},"source":["## Initial Imports\r\n","We will first have two imports: TensorFlow and Pandas."]},{"cell_type":"code","metadata":{"id":"QDblVt5TaNFI","executionInfo":{"status":"ok","timestamp":1614676258214,"user_tz":-60,"elapsed":74459,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}}},"source":["import tensorflow as tf\r\n","import pandas as pd"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F776Vz23dQgL","executionInfo":{"status":"ok","timestamp":1614676258219,"user_tz":-60,"elapsed":74456,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"d2710a9e-781f-45ed-896a-d3e2709288a8"},"source":["myDataPath = '/gdrive/MyDrive/Colab Notebooks/Transformers/data/'\r\n","!ls {myDataPath.replace(' ', '\\ ')} -lh"],"execution_count":8,"outputs":[{"output_type":"stream","text":["total 221M\n","-rw------- 1 root root 6.4M Feb 25 18:19 aclImdb_v1_test.csv\n","-rw------- 1 root root  26M Feb 25 18:19 aclImdb_v1_train.csv\n","-rw------- 1 root root  58M Feb 24 09:30 ToxicComments_test.csv\n","-rw------- 1 root root  66M Feb 24 09:30 ToxicComments_train_conv.csv\n","-rw------- 1 root root  66M Feb 24 09:30 ToxicComments_train.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"Q0huwS3ldQgO","executionInfo":{"status":"ok","timestamp":1614676259175,"user_tz":-60,"elapsed":75397,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"58998149-8ee1-4bb9-a223-f75b1bf52a85"},"source":["test = pd.read_csv(myDataPath + 'aclImdb_v1_test.csv', sep=\"|\")\r\n","test"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>LABEL_COLUMN</th>\n","      <th>DATA_COLUMN</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>I can't believe that so much talent can be was...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>This movie blows - let's get that straight rig...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>The saddest thing about this \"tribute\" is that...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>I'm only rating this film as a 3 out of pity b...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>Something surprised me about this movie - it w...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4995</th>\n","      <td>1</td>\n","      <td>Maybe one of the most entertaining Ninja-movie...</td>\n","    </tr>\n","    <tr>\n","      <th>4996</th>\n","      <td>0</td>\n","      <td>Sometimes, making something strange and contem...</td>\n","    </tr>\n","    <tr>\n","      <th>4997</th>\n","      <td>1</td>\n","      <td>If you like cars you will love this film!&lt;br /...</td>\n","    </tr>\n","    <tr>\n","      <th>4998</th>\n","      <td>1</td>\n","      <td>Our imp of the perverse did good his first tim...</td>\n","    </tr>\n","    <tr>\n","      <th>4999</th>\n","      <td>1</td>\n","      <td>OK, this movie is stupid. I mean that in a goo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5000 rows × 2 columns</p>\n","</div>"],"text/plain":["      LABEL_COLUMN                                        DATA_COLUMN\n","0                0  I can't believe that so much talent can be was...\n","1                0  This movie blows - let's get that straight rig...\n","2                0  The saddest thing about this \"tribute\" is that...\n","3                0  I'm only rating this film as a 3 out of pity b...\n","4                1  Something surprised me about this movie - it w...\n","...            ...                                                ...\n","4995             1  Maybe one of the most entertaining Ninja-movie...\n","4996             0  Sometimes, making something strange and contem...\n","4997             1  If you like cars you will love this film!<br /...\n","4998             1  Our imp of the perverse did good his first tim...\n","4999             1  OK, this movie is stupid. I mean that in a goo...\n","\n","[5000 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"T0StPUZMd6n-","executionInfo":{"status":"ok","timestamp":1614676260232,"user_tz":-60,"elapsed":76446,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"193cf3d6-9104-4bcf-a6e9-8ddef7bdf98a"},"source":["train = pd.read_csv(myDataPath + 'aclImdb_v1_train.csv', sep=\"|\")\r\n","train"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>LABEL_COLUMN</th>\n","      <th>DATA_COLUMN</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Canadian director Vincenzo Natali took the art...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>I gave this film 10 not because it is a superb...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>I admit to being somewhat jaded about the movi...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>For a long time, 'The Menagerie' was my favori...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>A truly frightening film. Feels as if it were ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>19995</th>\n","      <td>1</td>\n","      <td>Well this movie was probobly one of the funnie...</td>\n","    </tr>\n","    <tr>\n","      <th>19996</th>\n","      <td>1</td>\n","      <td>I love this movie, but can't get what is in th...</td>\n","    </tr>\n","    <tr>\n","      <th>19997</th>\n","      <td>1</td>\n","      <td>&lt;br /&gt;&lt;br /&gt;Superb film with no actual spoken ...</td>\n","    </tr>\n","    <tr>\n","      <th>19998</th>\n","      <td>1</td>\n","      <td>David Beckham is a British soccer star and the...</td>\n","    </tr>\n","    <tr>\n","      <th>19999</th>\n","      <td>0</td>\n","      <td>Four porn stars romping through the Irish wood...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20000 rows × 2 columns</p>\n","</div>"],"text/plain":["       LABEL_COLUMN                                        DATA_COLUMN\n","0                 1  Canadian director Vincenzo Natali took the art...\n","1                 1  I gave this film 10 not because it is a superb...\n","2                 1  I admit to being somewhat jaded about the movi...\n","3                 1  For a long time, 'The Menagerie' was my favori...\n","4                 0  A truly frightening film. Feels as if it were ...\n","...             ...                                                ...\n","19995             1  Well this movie was probobly one of the funnie...\n","19996             1  I love this movie, but can't get what is in th...\n","19997             1  <br /><br />Superb film with no actual spoken ...\n","19998             1  David Beckham is a British soccer star and the...\n","19999             0  Four porn stars romping through the Irish wood...\n","\n","[20000 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"OY__mmNzbFPs"},"source":["## Creating Input Sequences\r\n","We have two pandas Dataframe objects waiting for us to convert them into suitable objects for the BERT model. We will take advantage of the InputExample function that helps us to create sequences from our dataset. The InputExample function can be called as follows:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YgcKjdFIbCAL","executionInfo":{"status":"ok","timestamp":1614676260238,"user_tz":-60,"elapsed":76446,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"bf167f13-5110-4b4a-f2ec-47a25036b7c8"},"source":["# transformers.InputExample\r\n","InputExample(guid=None,\r\n","             text_a = \"Hello, world\",\r\n","             text_b = None,\r\n","             label = 1)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["InputExample(guid=None, text_a='Hello, world', text_b=None, label=1)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"jbU6diuYbNVM"},"source":["Now we will create two main functions:\r\n","\r\n","1 — `convert_data_to_examples`: This will accept our train and test datasets and convert each row into an InputExample object.\r\n","\r\n","2 — `convert_examples_to_tf_dataset`: This function will tokenize the InputExample objects, then create the required input format with the tokenized objects, finally, create an input dataset that we can feed to the model."]},{"cell_type":"code","metadata":{"id":"ZobDj7ZibI78","executionInfo":{"status":"ok","timestamp":1614676260242,"user_tz":-60,"elapsed":76443,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}}},"source":["def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \r\n","  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\r\n","                                                          text_a = x[DATA_COLUMN], \r\n","                                                          text_b = None,\r\n","                                                          label = x[LABEL_COLUMN]), axis = 1)\r\n","\r\n","  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\r\n","                                                          text_a = x[DATA_COLUMN], \r\n","                                                          text_b = None,\r\n","                                                          label = x[LABEL_COLUMN]), axis = 1)\r\n","  \r\n","  return train_InputExamples, validation_InputExamples  "],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wUBIWuEbleM","executionInfo":{"status":"ok","timestamp":1614676260246,"user_tz":-60,"elapsed":76444,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}}},"source":["train_InputExamples, validation_InputExamples = convert_data_to_examples(train, \r\n","                                                                           test, \r\n","                                                                           'DATA_COLUMN', \r\n","                                                                           'LABEL_COLUMN')"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"k52dErBYbVst","executionInfo":{"status":"ok","timestamp":1614676260250,"user_tz":-60,"elapsed":76444,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}}},"source":["def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\r\n","    features = [] # -> will hold InputFeatures to be converted later\r\n","\r\n","    for e in examples:\r\n","        # Documentation is really strong for this method, so please take a look at it\r\n","        input_dict = tokenizer.encode_plus(\r\n","            e.text_a,\r\n","            add_special_tokens=True,\r\n","            max_length=max_length, # truncates if len(s) > max_length\r\n","            return_token_type_ids=True,\r\n","            return_attention_mask=True,\r\n","            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\r\n","            truncation=True\r\n","        )\r\n","\r\n","        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\r\n","            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\r\n","\r\n","        features.append(\r\n","            InputFeatures(\r\n","                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\r\n","            )\r\n","        )\r\n","\r\n","    def gen():\r\n","        for f in features:\r\n","            yield (\r\n","                {\r\n","                    \"input_ids\": f.input_ids,\r\n","                    \"attention_mask\": f.attention_mask,\r\n","                    \"token_type_ids\": f.token_type_ids,\r\n","                },\r\n","                f.label,\r\n","            )\r\n","\r\n","    return tf.data.Dataset.from_generator(\r\n","        gen,\r\n","        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\r\n","        (\r\n","            {\r\n","                \"input_ids\": tf.TensorShape([None]),\r\n","                \"attention_mask\": tf.TensorShape([None]),\r\n","                \"token_type_ids\": tf.TensorShape([None]),\r\n","            },\r\n","            tf.TensorShape([]),\r\n","        ),\r\n","    )\r\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"n44iQfkQba9M","executionInfo":{"status":"ok","timestamp":1614676260253,"user_tz":-60,"elapsed":76444,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}}},"source":["DATA_COLUMN = 'DATA_COLUMN'\r\n","LABEL_COLUMN = 'LABEL_COLUMN'"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JsahLWQIfsMv","executionInfo":{"status":"ok","timestamp":1614676260255,"user_tz":-60,"elapsed":76439,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"5d855509-f5af-409b-e339-f22db18b4ea1"},"source":["print (str(type(DATA_COLUMN)) + ' ' + DATA_COLUMN)\r\n","print (str(type(LABEL_COLUMN)) + ' ' + LABEL_COLUMN)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["<class 'str'> DATA_COLUMN\n","<class 'str'> LABEL_COLUMN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"GzSyFUoKgbNE","executionInfo":{"status":"ok","timestamp":1614676260621,"user_tz":-60,"elapsed":76795,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"c9e1342b-cf59-4e34-ca32-912fd0061cd4"},"source":["train.head(5)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>LABEL_COLUMN</th>\n","      <th>DATA_COLUMN</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Canadian director Vincenzo Natali took the art...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>I gave this film 10 not because it is a superb...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>I admit to being somewhat jaded about the movi...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>For a long time, 'The Menagerie' was my favori...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>A truly frightening film. Feels as if it were ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   LABEL_COLUMN                                        DATA_COLUMN\n","0             1  Canadian director Vincenzo Natali took the art...\n","1             1  I gave this film 10 not because it is a superb...\n","2             1  I admit to being somewhat jaded about the movi...\n","3             1  For a long time, 'The Menagerie' was my favori...\n","4             0  A truly frightening film. Feels as if it were ..."]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BR_puwpBbtid","executionInfo":{"status":"ok","timestamp":1614676375793,"user_tz":-60,"elapsed":191960,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"60fbde8b-efef-40cf-988f-01bc1e3a5107"},"source":["%%time\r\n","\r\n","train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)\r\n","\r\n","train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\r\n","train_data = train_data.shuffle(100).batch(32).repeat(2)\r\n","\r\n","validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\r\n","validation_data = validation_data.batch(32)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["CPU times: user 1min 54s, sys: 178 ms, total: 1min 55s\n","Wall time: 1min 55s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qe-9PgI_ckZv"},"source":["## Configuring the BERT model and Fine-tuning\r\n","We will use Adam as our optimizer, CategoricalCrossentropy as our loss function, and SparseCategoricalAccuracy as our accuracy metric. Fine-tuning the model for 2 epochs will give us around 95% accuracy, which is great."]},{"cell_type":"code","metadata":{"id":"bxypaKh8cg3m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614678645435,"user_tz":-60,"elapsed":2461595,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"d7cc7e0a-afcb-471d-bd16-2eaa9a2f3097"},"source":["%%time\r\n","\r\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \r\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \r\n","              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\r\n","\r\n","model.fit(train_data, epochs=2, validation_data=validation_data)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f9199cf9e50>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f9199cf9e50>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f91b5124c20> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <function wrap at 0x7f91b5124c20> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","   1250/Unknown - 1104s 845ms/step - loss: 0.3702 - accuracy: 0.8260WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","1250/1250 [==============================] - 1149s 881ms/step - loss: 0.3702 - accuracy: 0.8260 - val_loss: 0.3352 - val_accuracy: 0.8836\n","Epoch 2/2\n","1250/1250 [==============================] - 1120s 896ms/step - loss: 0.1024 - accuracy: 0.9636 - val_loss: 0.4690 - val_accuracy: 0.8766\n","CPU times: user 13min 22s, sys: 13min 46s, total: 27min 9s\n","Wall time: 37min 49s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pS9SoNm-dD07"},"source":["Training the model might take a while, so ensure you enabled the GPU acceleration from the Notebook Settings. After our training is completed, we can move onto making sentiment predictions."]},{"cell_type":"markdown","metadata":{"id":"G2kIo1BtdOai"},"source":["## Making Predictions\r\n","I created a list of two reviews I created. The first one is a positive review, while the second one is clearly negative."]},{"cell_type":"code","metadata":{"id":"w2HLb5gFcpL-","executionInfo":{"status":"ok","timestamp":1614678645444,"user_tz":-60,"elapsed":2461600,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}}},"source":["pred_sentences = ['This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good',\r\n","                  'One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie',\r\n","                  'A truly frightening film.',\r\n","                  'What a waste of time.']"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rM2YzQrmdXUa"},"source":["We need to tokenize our reviews with our pre-trained BERT tokenizer. We will then feed these tokenized sequences to our model and run a final softmax layer to get the predictions. We can then use the argmax function to determine whether our sentiment prediction for the review is positive or negative. Finally, we will print out the results with a simple for loop. The following lines do all of these said operations:"]},{"cell_type":"code","metadata":{"id":"lxD4gyExdTZ6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614678645450,"user_tz":-60,"elapsed":2461592,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"aef0de8c-9064-4c65-833f-ddbc811ef657"},"source":["tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\r\n","tf_outputs = model(tf_batch)\r\n","tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\r\n","labels = ['Negative','Positive']\r\n","label = tf.argmax(tf_predictions, axis=1)\r\n","label = label.numpy()\r\n","for i in range(len(pred_sentences)):\r\n","  print(pred_sentences[i], \": \\n\", labels[label[i]] +\" with score: \"+ str(tf_predictions[i][label[i]].numpy()))\r\n","  print ()"],"execution_count":21,"outputs":[{"output_type":"stream","text":["This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good : \n"," Positive with score: 0.9982886\n","\n","One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie : \n"," Negative with score: 0.99944216\n","\n","A truly frightening film. : \n"," Positive with score: 0.9975387\n","\n","What a waste of time. : \n"," Negative with score: 0.99877983\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NUd8tmWUnK28"},"source":["## Debugging the Final Tensor Shape"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDRGxQh_V8bt","executionInfo":{"status":"ok","timestamp":1614678645453,"user_tz":-60,"elapsed":2461587,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"4635768b-564c-4268-b82a-ed961aa33abc"},"source":["tf_predictions.shape"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([4, 2])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CRDxUOJmgvK4","executionInfo":{"status":"ok","timestamp":1614678645455,"user_tz":-60,"elapsed":2461582,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"c7b2c262-ec70-47bc-9066-115b0539b5b1"},"source":["for i in range(len(tf_predictions)):\r\n","  print (tf_predictions[i])"],"execution_count":23,"outputs":[{"output_type":"stream","text":["tf.Tensor([0.00171144 0.9982886 ], shape=(2,), dtype=float32)\n","tf.Tensor([9.9944216e-01 5.5781094e-04], shape=(2,), dtype=float32)\n","tf.Tensor([0.00246128 0.9975387 ], shape=(2,), dtype=float32)\n","tf.Tensor([0.99877983 0.00122019], shape=(2,), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cjcFubhmhFQB","executionInfo":{"status":"ok","timestamp":1614678645457,"user_tz":-60,"elapsed":2461577,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"d180fbf2-aff9-43fd-f6d0-3a06fedf7537"},"source":["for i in range(len(tf_predictions)):\r\n","  print (str(tf_predictions[i][0]) + ' - ' + str(tf_predictions[i][1]))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["tf.Tensor(0.0017114393, shape=(), dtype=float32) - tf.Tensor(0.9982886, shape=(), dtype=float32)\n","tf.Tensor(0.99944216, shape=(), dtype=float32) - tf.Tensor(0.00055781094, shape=(), dtype=float32)\n","tf.Tensor(0.002461283, shape=(), dtype=float32) - tf.Tensor(0.9975387, shape=(), dtype=float32)\n","tf.Tensor(0.99877983, shape=(), dtype=float32) - tf.Tensor(0.0012201883, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q88Ppt5Dh9yb","executionInfo":{"status":"ok","timestamp":1614678645459,"user_tz":-60,"elapsed":2461573,"user":{"displayName":"Dirkster9999 ___","photoUrl":"","userId":"14481456906440972157"}},"outputId":"d6187316-f847-4a1b-b8e6-84531671d710"},"source":["for i in range(len(tf_predictions)):\r\n","  print(tf_predictions[i][label[i]].numpy())"],"execution_count":25,"outputs":[{"output_type":"stream","text":["0.9982886\n","0.99944216\n","0.9975387\n","0.99877983\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oBDcRsbWt74i"},"source":["Also, with the code above, you can predict as many reviews as possible.\r\n","\r\n","# Congratulations\r\n","\r\n","You have successfully built a transformers network with a pre-trained BERT model and achieved ~95% accuracy on the sentiment analysis of the IMDB reviews dataset! If you are curious about saving your model, I would like to direct you to the Keras Documentation. After all, to efficiently use an API, one must learn how to read and use the documentation."]}]}